{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentiment.py\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle as pkl\n",
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "from pymorphy2.analyzer import MorphAnalyzer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_emotional_columns(x):\n",
    "    emotions = {')', '!', '('}\n",
    "    for e in emotions:\n",
    "        if e in x:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "lemmatizer = MorphAnalyzer()\n",
    "\n",
    "\n",
    "bad_words = {'–±–æ–º–∂', '–µ–±—É—á–∏–π', '–∞–¥', '–ø–∏–¥–∞—Ä–∞—Å–∏–Ω–∞', '–æ—Ç—Å–æ—Å–∏', '—à–ª—é—Ö–∞', '–Ω–∞—Ö—É–π', '–∫–æ–∫—Å', '–ø–∏–¥–æ—Ä', '–≥–∞–¥–∫–∏–π',\n",
    "'–ª–æ—Ö', '–ø–∏–¥—Ä', '–ø–æ—à–ª–Ω—Ö', '—É—Ä–æ–¥', '—Ö—É–π', '–ø–æ–¥–∞–≤–∏—Å—å', '–ø–∑–¥—Ü', '–∂–æ–ø–∞', '–ø–µ–¥–∏–∫' '–∑–∞–µ–±–∞–ª', '–µ–±—É—á–∏–π', '–ø–∏–∑–¥–µ—Ü',\n",
    "'–≥–∞–Ω–¥–æ–Ω', '–¥–æ–ª–±–æ–µ–±', '—Ö—É–µ—Å–æ—Å', '–º—Ä–∞–∑—å', '—Å–∫–æ—Ç–∏–Ω–∞', '–≥–Ω–∏–¥–∞', '–ø—Ä–æ—Å—Ç–∏—Ç—É—Ç–∫–∞', '–º–∞–Ω–¥–∞', '–±–ª—è', '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä',\n",
    "'–≤–∑—è—Ç–∫–∞', '–æ—Ç–∫–∞—Ç', '—Ç–µ—Ä—Ä–æ—Ä', '—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º', '–≤–∑—Ä—ã–≤', '–±–æ–º–±–∞', '–¥—Ä–∞–∫–∞', '—É–±–∏–π—Å—Ç–≤–æ', '—Ç—Ä—É–ø', '—É–±–∏—Ç—å',\n",
    "'–µ–±–∞—Ç—å', '—Ä–∞–∑—ä–µ–±–∞—Ç—å', '–≤—ä–µ–±–∞—Ç—å', '—á–ª–µ–Ω'}\n",
    " \n",
    "good_words = {'—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∑–¥—Ä–∞–≤–ª—è—é', '–ª—é–±–ª—é', '–º–∏–ª–∞—è', '–º–∏–ª—ã–π',\n",
    "'–ø—Ä–∞–∑–¥–Ω–∏–∫', '—Ä–æ–∂–¥–µ–Ω–∏—è', '–∑–¥–æ—Ä–æ–≤—å—è', '–ª—é–±–∏–º–∞—è', '–º—É–∂', '–∫–æ—Ç–∏–∫', '–∫–æ—Ç'\n",
    "'—Ü–µ–ª—É—é', '—Å–ø—Å', '–¥—Ä', '–¥–Ω—é—Ö–∞', '–±–ª–∞–≥–æ–¥–∞—Ä—é', '‚ù§', 'üòÇ', '—Å–ª–∞–¥–æ—Å—Ç–∏', 'üòô', '–∑–∞—è',\n",
    "'–¥–æ—á–µ–Ω—å–∫–∞', 'üòä', 'üòÄ', '—Å—ã–Ω–æ–∫', 'üòò', 'üòç', '—Ö–æ—Ä–æ—à–∏–π', '‚ù§', 'üòâ', '–º–∞–º–∞', '–±–æ–≥', '–∑–¥–æ—Ä–æ–≤—å–µ',\n",
    "'–¥—Ä—É–≥', 'üëã', '–±—Ä–∞—Ç', '–¥–µ—Ç–∫–∞', 'üòΩ', 'üòã', '—á–º–æ–∫', '—Å–ø–∞—Å', '–∫—Ä–∞—Å–∞–≤—á–∏–∫', 'üòà',\n",
    "'—Å–≤–∞–¥—å–±–∞', '—Å—á–∞—Å—Ç—å–µ', 'üí™', 'üòÅ', 'üòé', 'üí∞', '–¥—Ä—É–≥', '–¥—Ä—É–∂–æ—á–µ–∫', '–±–∏—Å–º–∏–ª–ª–∞—Ö', '–ø–∞–ø–∞', 'üòã', '–ª—é–±–∏–º–æ–π',\n",
    "'—Ä—ç—Ö–º—ç—Ç', '–ø—Ä–∏—è—Ç–Ω—ã–π', '–ª—é–±–æ–≤—å', '–¥–æ—Ä–æ–≥–∞—è', '–¥–æ—Ä–æ–≥–æ–π', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', 'üòÇ', '–ª—é–±–∏–º–æ–π', '–∫—Ä–∞—Å–æ—Ç–∞', 'ü§ó',\n",
    "'–¥–æ—á—å'}\n",
    "\n",
    "def razmetka_positive(x):\n",
    "    x = x.lower().split()\n",
    "    for i in range(len(x)):\n",
    "        x[i] = ''.join(e for e in x[i] if e.isalnum())\n",
    "        \n",
    "    lemmatized_word = set()\n",
    "    global good_words\n",
    "    for w in x:\n",
    "        lemm = lemmatizer.parse(w)[0].normal_form\n",
    "        lemmatized_word.add(w)\n",
    "    \n",
    "    intersect = len(lemmatized_word & good_words)\n",
    "    \n",
    "    if intersect > 0:\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "    \n",
    "    \n",
    "def razmetka_negative(x):\n",
    "    x = x.lower().split()\n",
    "    for i in range(len(x)):\n",
    "        x[i] = ''.join(e for e in x[i] if e.isalnum())\n",
    "        \n",
    "    lemmatized_word = set()\n",
    "    global bad_words\n",
    "    for w in x:\n",
    "        lemm = lemmatizer.parse(w)[0].normal_form\n",
    "        lemmatized_word.add(w)\n",
    "    \n",
    "    intersect = len(lemmatized_word & bad_words)\n",
    "    \n",
    "    if intersect > 0:\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('new_data.pkl')\n",
    "\n",
    "data['emotional_column'] = data['message'].apply(add_emotional_columns)\n",
    "data['pos_target'] = data['message'].apply(razmetka_positive)\n",
    "data['neg_target'] = data['message'].apply(razmetka_negative)\n",
    "\n",
    "data.drop('target',axis=1,inplace=True)\n",
    "\n",
    "data.to_csv('training_data_with_razmetka_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "tokenizer = RegexTokenizer()\n",
    "model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "\n",
    "# morph analyzer for text lemmatization\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "fasttext = FastTextKeyedVectors.load('187/model.model')\n",
    "pos_log_reg = pkl.load(open('pos_log_reg.pkl', 'rb'))\n",
    "neg_log_reg = pkl.load(open('neg_log_reg.pkl', 'rb'))\n",
    "pos_log_reg_dost = pkl.load(open('pos_log_reg_dost.pkl', 'rb'))\n",
    "neg_log_reg_dost = pkl.load(open('neg_log_reg_dost.pkl', 'rb'))\n",
    "\n",
    "old_data = pd.read_pickle('data/new_data.pkl')\n",
    "old_data['index'] = old_data.index\n",
    "\n",
    "training_data = pd.read_csv('data/training_data_with_razmetka_final.csv')\n",
    "\n",
    "data_new = training_data.merge(old_data, on=['index','message'])\n",
    "cut_date = lambda x: datetime.date(x.year, x.month, x.day)\n",
    "data_new['local_datetime'] = pd.to_datetime(data_new.local_datetime).apply(cut_date)\n",
    "\n",
    "# function for performing parallel computing on cpu\n",
    "def parallelization(func, massive, jobs=None, tq=True):\n",
    "    num_cores = multiprocessing.cpu_count() if jobs is None else jobs\n",
    "    if tq:\n",
    "        results = np.array(Parallel(n_jobs=num_cores)(delayed(func)(i) for i in tqdm(massive)))\n",
    "        return results\n",
    "    else:\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(func)(i) for i in massive)\n",
    "        return results\n",
    "\n",
    "\n",
    "def _word2canonical4w2v(word):\n",
    "    elems = morph.parse(word)\n",
    "    my_tag = ''\n",
    "    res = []\n",
    "    for elem in elems:\n",
    "        if 'VERB' in elem.tag or 'GRND' in elem.tag or 'INFN' in elem.tag:\n",
    "            my_tag = 'V'\n",
    "        if 'NOUN' in elem.tag:\n",
    "            my_tag = 'S'\n",
    "        normalised = elem.normalized.word\n",
    "        res.append((normalised, my_tag))\n",
    "    tmp = list(filter(lambda x: x[1] != '', res))\n",
    "    if len(tmp) > 0:\n",
    "        return tmp[0]\n",
    "    else:\n",
    "        return res[0]\n",
    "\n",
    "\n",
    "def word2canonical(word):\n",
    "    return _word2canonical4w2v(word)[0]\n",
    "\n",
    "\n",
    "def get_words(text, filter_short_words=False):\n",
    "    if filter_short_words:\n",
    "        return filter(lambda x: len(x) > 2, re.findall('[–∞-—è–ê-–Øa-zA-Z]+', text))#re.findall(r'(?u)\\w+', text))\n",
    "    else:\n",
    "        return re.findall(r'(?u)\\w+', text)\n",
    "\n",
    "def text2canonicals(text, add_word=False, filter_short_words=True):\n",
    "    words = []\n",
    "    for word in get_words(text, filter_short_words=filter_short_words):\n",
    "        words.append(word2canonical(word.lower()))\n",
    "        if add_word:\n",
    "            words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_text_vectors(text):\n",
    "    matrix = np.zeros((len(text), 300))\n",
    "    for i,word in enumerate(text):\n",
    "        vector = fasttext[word]\n",
    "        matrix[i] = vector\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "def get_dost_vector(pred):\n",
    "    return np.array([pred['positive'], pred['skip'], pred['speech'], pred['neutral'], pred['positive']])\n",
    "\n",
    "\n",
    "def preprocess(texts):\n",
    "    # embedding vectors weighted with tfidf\n",
    "    preprocessed_texts = parallelization(text2canonicals, texts)\n",
    "    lengths = np.array(list(map(lambda x: len(x) if len(x) > 0 else 1, preprocessed_texts)))\n",
    "    \n",
    "    texts = list(map(lambda x: ' '.join(x), preprocessed_texts))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfifd_vectorized = vectorizer.fit_transform(texts).toarray()\n",
    "    unique_words = list(map(lambda x: x[0], sorted(vectorizer.vocabulary_.items())))\n",
    "    \n",
    "    all_vectors = get_text_vectors(unique_words)\n",
    "    weighted_embeddings = tfifd_vectorized @ all_vectors\n",
    "    weighted_embeddings /= lengths.reshape(-1, 1)\n",
    "    del tfifd_vectorized, all_vectors\n",
    "    \n",
    "    return weighted_embeddings\n",
    "\n",
    "def preprocess_single_text(text):\n",
    "    # embedding vectors weighted with tfidf\n",
    "    preprocessed_text = text2canonicals(text)\n",
    "    length = len(preprocessed_text) if len(x) > 0 else 1\n",
    "    \n",
    "    preprocessed_text = ' '.join(preprocessed_text)\n",
    "    vectorizer = pkl.load(open('vectorizer.pkl', 'wb'))\n",
    "    tfifd_vectorized = vectorizer.transform([text]).toarray()\n",
    "    unique_words = list(map(lambda x: x[0], sorted(vectorizer.vocabulary_.items())))\n",
    "    \n",
    "    all_vectors = get_text_vectors(unique_words)\n",
    "    weighted_embeddings = tfifd_vectorized @ all_vectors\n",
    "    weighted_embeddings /= length\n",
    "    del tfifd_vectorized, all_vectors\n",
    "    \n",
    "    return weighted_embeddings\n",
    "\n",
    "\n",
    "def preprocess2(texts, use_dost=False):\n",
    "    # mean embedding vectors\n",
    "    if use_dost:\n",
    "        preds = model.predict(texts)\n",
    "        dost_vectors = np.array(list(map(get_dost_vector, preds)))\n",
    "    \n",
    "    #preprocessed_texts = parallelization(text2canonicals, texts)\n",
    "    preprocessed_texts = list(map(lambda x: x.lower().split(), texts))\n",
    "    \n",
    "    embeddings = np.zeros((len(texts), 300))\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        vectors = get_text_vectors(text)\n",
    "        if vectors.shape[0] > 0:\n",
    "            vector = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            vector = np.random.randn(300,)\n",
    "        embeddings[i] = vector\n",
    "        \n",
    "    if use_dost:\n",
    "        return np.concatenate((embeddings, dost_vectors), axis=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def emotional(x):\n",
    "    emotions = {')', '!', '('}\n",
    "    for e in emotions:\n",
    "        if e in x:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sentiment_analysis(sentences, use_dost=True):\n",
    "    emotional_col = list(map(lambda x: emotional(x), sentences))\n",
    "    prep_sent = preprocess2(sentences, True)\n",
    "    inputs = np.concatenate((prep_sent, np.array(emotional_col).reshape(-1, 1)), axis=1)\n",
    "    if use_dost:\n",
    "        preds_pos = list(map(lambda x: np.round(x[1], 3), pos_log_reg_dost.predict_proba(inputs)))\n",
    "        preds_neg = list(map(lambda x: np.round(x[1], 3), neg_log_reg_dost.predict_proba(inputs)))\n",
    "    else:\n",
    "        preds_pos = list(map(lambda x: np.round(x[1], 3), pos_log_reg.predict_proba(inputs)))\n",
    "        preds_neg = list(map(lambda x: np.round(x[1], 3), neg_log_reg.predict_proba(inputs)))\n",
    "    \n",
    "    return np.array([preds_pos, preds_neg]).T\n",
    "\n",
    "\n",
    "def get_date_list(numdays, base):\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n",
    "    return date_list\n",
    "\n",
    "\n",
    "def sent_analyse_dates(period, base=datetime.date(2019, 9, 3)):\n",
    "    if period == 'week':\n",
    "        date_list = get_date_list(7, base)\n",
    "        data_period = data_new[data_new.local_datetime.isin(date_list)]\n",
    "\n",
    "    elif period == 'month':\n",
    "        date_list = get_date_list(31, base)\n",
    "        data_period = data_new[data_new.local_datetime.isin(date_list)]\n",
    "\n",
    "    elif period == 'all':\n",
    "        data_period = data_new\n",
    "    else:\n",
    "        raise ValueError('invalid period name')\n",
    "        \n",
    "    predictions = sentiment_analysis(data_period.message.values)\n",
    "    return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl.dump(pos_log_reg, open('pos_log_reg_dost.pkl', 'wb'))\n",
    "#pkl.dump(neg_log_reg, open('neg_log_reg_dost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = list(training_data.index)\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:int(len(inds) * 0.8)]\n",
    "test_inds = inds[int(len(inds) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = training_data[training_data.index.isin(train_inds)]\n",
    "data_test = training_data[training_data.index.isin(test_inds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_targets_train = data_train.pos_target#np.array(data.label == 'positive').astype(np.int32)\n",
    "neg_targets_train = data_train.neg_target\n",
    "\n",
    "pos_targets_test = data_test.pos_target#np.array(data.label == 'positive').astype(np.int32)\n",
    "neg_targets_test = data_test.neg_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_train = preprocess2(data_train.message.values, use_dost=True)\n",
    "embeddings_test = preprocess2(data_test.message.values, use_dost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = np.concatenate((embeddings_train,\n",
    "                                      data_train.emotional_column.values.reshape(-1, 1)), axis=1)\n",
    "embeddings_test = np.concatenate((embeddings_test,\n",
    "                                      data_test.emotional_column.values.reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_log_reg = LogisticRegression()\n",
    "neg_log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_pos, y_train_neg =  embeddings_train, pos_targets_train, neg_targets_train\n",
    "X_test, y_test_pos, y_test_neg = embeddings_test, pos_targets_test, neg_targets_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_log_reg.fit(X_train, y_train_pos)\n",
    "neg_log_reg.fit(X_train, y_train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive train accuracy: 0.9841030663753485\n",
      "Positive test accuracy: 0.980864848576164\n",
      "\n",
      "Negative train accuracy: 0.9991712499058238\n",
      "Negative test accuracy: 0.9966852493596504\n"
     ]
    }
   ],
   "source": [
    "print('Positive train accuracy: {}'.format(pos_log_reg.score(X_train, y_train_pos)))\n",
    "print('Positive test accuracy: {}'.format(pos_log_reg.score(X_test, y_test_pos)))\n",
    "print()\n",
    "print('Negative train accuracy: {}'.format(neg_log_reg.score(X_train, y_train_neg)))\n",
    "print('Negative test accuracy: {}'.format(neg_log_reg.score(X_test, y_test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pos = list(map(lambda x: np.round(x[1], 3), pos_log_reg.predict_proba(X_test)))\n",
    "preds_neg = list(map(lambda x: np.round(x[1], 3), neg_log_reg.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_test['preds_pos'] = preds_pos\n",
    "data_test['preds_neg'] = preds_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>preds_pos</th>\n",
       "      <th>preds_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17374</th>\n",
       "      <td>–õ–∏–ª–∏—è</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32099</th>\n",
       "      <td>–ù–∞–ø–∏—à–∏, –∫–∞–∫ –ø—Ä–∏–¥—É—Ç</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12180</th>\n",
       "      <td>–†–∂–µ–≤—Å–∫–∏–π</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33100</th>\n",
       "      <td>–¥–æ–ª–≥</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>–ú–∞—Ä–∫—É—Ö–∏–Ω –ü–∞–≤–µ–ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12937</th>\n",
       "      <td>—é—Ä–∞—à–∏–∫</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32019</th>\n",
       "      <td>–ú–∞—Ü—É–∫–æ–≤–∞ –î–∞—Ä—å—è</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32029</th>\n",
       "      <td>–£ –ú–µ–Ω—è –û—Å—Ç–∞–ª–æ—Å—å -–∞.—Ä—É–±–ª—è.</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>–û—Ç –ï–ª–µ–Ω—ã –Æ</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>–ò—Ä–∏–Ω–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13005</th>\n",
       "      <td>–ë–µ–ª–µ–Ω–∫–æ –ï–≤–≥–µ–Ω–∏—è</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11383</th>\n",
       "      <td>–∑–∞ –∫–æ–Ω—Ñ–µ—Ç–∫–∏</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>–û—Ç –ê—Ä—Ç—É—Ä–∞</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>–∏–≥—Ä—ã</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13687</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23547</th>\n",
       "      <td>–ó–∞ –ú–∞—Å–ª–æ. –û–≥—Ä–æ–º–Ω–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ.</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>–ø—Ä–∏—à–ª–∏?</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30279</th>\n",
       "      <td>–°–ø–∞—Å–∏–±–æ)</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>–ü–æ—à—Ç–∞—Ä—å –ê–Ω–∞—Ç–æ–ª–∏–π</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21289</th>\n",
       "      <td>–û–ø–ª–∞—Ç–∞ –î–æ–ª–≥–∞</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13368</th>\n",
       "      <td>test -</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9575</th>\n",
       "      <td>–ö –û—Ç–ø—É—Å–∫—É –ë—Ä–∞—Ç–∏—à)))</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>–º–∞–º–∞</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23305</th>\n",
       "      <td>–ë—Ä–∞—Ç—É–ª–µ—Ü, –∏–∑–≤–∏–Ω–∏ —á—Ç–æ —Ç–∞–∫ –∑–∞—Ç—è–Ω—É–ª...</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28453</th>\n",
       "      <td>–ù–∞ –ì—Ä–µ—á–∫—É</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18884</th>\n",
       "      <td>–ø—Ä–æ–≤–µ—Ä–∫–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31689</th>\n",
       "      <td>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–∞—Ä—Ç—É –ú–ò–†</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12922</th>\n",
       "      <td>–ï–ª–µ–Ω–∞</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30935</th>\n",
       "      <td>–∑–∞ –∞–ª–ª–∞–¥–∏–Ω</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>–ü—Ä–æ–≤–µ—Ä–∫–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31420</th>\n",
       "      <td>Oleg Rabetsky</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29658</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12795</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26750</th>\n",
       "      <td>–û—Ç –ö—É–¥–æ—è—Ä–æ–≤–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32985</th>\n",
       "      <td>–ì—Ä—É–∑–Ω–æ–≤–∞ –ù, –ì—Ä—É–∑–Ω–æ–≤–∞ –Æ</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29158</th>\n",
       "      <td>–®—Ç–æ—Ä—ã</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>–í–∏–∫—Ç–æ—Ä–∏—è –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤–Ω–∞</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24851</th>\n",
       "      <td>–î–µ–Ω—å–≥–∏</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>—Å–±–ø</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13311</th>\n",
       "      <td>–ú–µ–ª–∫–æ–≤ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23338</th>\n",
       "      <td>–Ω–∞ –≥–∞–∑–∑–∑–∑</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24694</th>\n",
       "      <td>–ø—Ä–æ–±–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>—Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥—Ä—É–∂–æ—á–µ–∫</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11057</th>\n",
       "      <td>–°–∞—à–∞–ö–æ–∫ —à–ø–∞–ª–∞</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24102</th>\n",
       "      <td>–ò–ª—å—è –ë–ª—É–∫–∞—á</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12774</th>\n",
       "      <td>–ö–∞—Ç—é—à–∫–∞, —Å–ø–∞—Å–∏–±–æ. —ç—Ç–æ –õ—é–¥–∞.</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11854</th>\n",
       "      <td>–ø—Ä–∏–≤–µ—Ç</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>–∑–∞ —Ñ—É—Ç–±–æ–ª–∫–∏</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>–ü—Ä–æ–±–Ω—ã–π –ü–µ—Ä–µ–≤–æ–¥</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30899</th>\n",
       "      <td>–î–æ–ª–≥</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29459</th>\n",
       "      <td>–¢–∞—Ç—å—è–Ω–∞ –§—ë–¥–æ—Ä–æ–≤–Ω–∞</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8070</th>\n",
       "      <td>–ó–∞ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤—É –ö–æ–Ω—Ç—É—Ä–Ω—ã–µ –ö–∞—Ä—Ç—ã</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17683</th>\n",
       "      <td>–í –≠—Ç–æ–º –ú–µ—Å—è—Ü–µ - –ø–æ —á–∞—Å—Ç—è–º. –ù. –ú.</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>–¥–æ–ª–∂–æ–∫.</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>–ó–∞ –ø–∏—Ç–∞–Ω–∏–µ –ö—Ä—é–∫–æ–≤–∞  —á–µ–ª–æ–≤–µ–∫–∞</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7918</th>\n",
       "      <td>–°–ø–∞—Å–∏–±–æ ##;##;</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24045</th>\n",
       "      <td>–ó–∞ –ø—Ä—è–Ω–∏–∫–∏  –Ω–∞–±–æ—Ä–æ–≤</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>–ì–æ—Ä–æ–¥–Ω—è–Ω—Å–∫–∏–π</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>–ó–∞ –∫—Ä—ã–ª—å—è))</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>—Ç–∞–∫—Å–∏</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   message  preds_pos  preds_neg\n",
       "17374                               –õ–∏–ª–∏—è       0.004      0.003\n",
       "32099                   –ù–∞–ø–∏—à–∏, –∫–∞–∫ –ø—Ä–∏–¥—É—Ç      0.006      0.000\n",
       "12180                             –†–∂–µ–≤—Å–∫–∏–π      0.000      0.001\n",
       "33100                                 –¥–æ–ª–≥      0.003      0.000\n",
       "7298          –ú–∞—Ä–∫—É—Ö–∏–Ω –ü–∞–≤–µ–ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á      0.000      0.000\n",
       "12937                               —é—Ä–∞—à–∏–∫      0.005      0.000\n",
       "32019                       –ú–∞—Ü—É–∫–æ–≤–∞ –î–∞—Ä—å—è      0.001      0.000\n",
       "32029            –£ –ú–µ–Ω—è –û—Å—Ç–∞–ª–æ—Å—å -–∞.—Ä—É–±–ª—è.      0.010      0.000\n",
       "8007                            –û—Ç –ï–ª–µ–Ω—ã –Æ      0.003      0.000\n",
       "1617                                 –ò—Ä–∏–Ω–∞      0.000      0.000\n",
       "13005                      –ë–µ–ª–µ–Ω–∫–æ –ï–≤–≥–µ–Ω–∏—è      0.001      0.000\n",
       "11383                          –∑–∞ –∫–æ–Ω—Ñ–µ—Ç–∫–∏      0.034      0.000\n",
       "29994                            –û—Ç –ê—Ä—Ç—É—Ä–∞      0.007      0.000\n",
       "26013                                 –∏–≥—Ä—ã      0.000      0.000\n",
       "13687                              –ü—Ä–∏–≤–µ—Ç       0.003      0.000\n",
       "23547        –ó–∞ –ú–∞—Å–ª–æ. –û–≥—Ä–æ–º–Ω–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ.       0.112      0.000\n",
       "5929                               –ø—Ä–∏—à–ª–∏?      0.003      0.000\n",
       "30279                             –°–ø–∞—Å–∏–±–æ)      1.000      0.000\n",
       "4870                      –ü–æ—à—Ç–∞—Ä—å –ê–Ω–∞—Ç–æ–ª–∏–π      0.000      0.001\n",
       "21289                         –û–ø–ª–∞—Ç–∞ –î–æ–ª–≥–∞      0.001      0.000\n",
       "13368                               test -      0.000      0.000\n",
       "9575                  –ö –û—Ç–ø—É—Å–∫—É –ë—Ä–∞—Ç–∏—à)))       0.030      0.001\n",
       "22505                                 –º–∞–º–∞      1.000      0.000\n",
       "23305  –ë—Ä–∞—Ç—É–ª–µ—Ü, –∏–∑–≤–∏–Ω–∏ —á—Ç–æ —Ç–∞–∫ –∑–∞—Ç—è–Ω—É–ª...      0.074      0.000\n",
       "28453                            –ù–∞ –ì—Ä–µ—á–∫—É      0.054      0.004\n",
       "18884                             –ø—Ä–æ–≤–µ—Ä–∫–∞      0.000      0.000\n",
       "31689       –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∫–∞—Ä—Ç—É –ú–ò–†      0.000      0.000\n",
       "12922                                –ï–ª–µ–Ω–∞      0.004      0.000\n",
       "30935                           –∑–∞ –∞–ª–ª–∞–¥–∏–Ω      0.004      0.000\n",
       "23614                             –ü—Ä–æ–≤–µ—Ä–∫–∞      0.000      0.000\n",
       "...                                    ...        ...        ...\n",
       "31420                        Oleg Rabetsky      0.000      0.000\n",
       "29658                                 Test      0.000      0.000\n",
       "12795                               –ü—Ä–∏–≤–µ—Ç      0.003      0.000\n",
       "26750                         –û—Ç –ö—É–¥–æ—è—Ä–æ–≤–∞      0.000      0.000\n",
       "32985               –ì—Ä—É–∑–Ω–æ–≤–∞ –ù, –ì—Ä—É–∑–Ω–æ–≤–∞ –Æ      0.001      0.000\n",
       "29158                                –®—Ç–æ—Ä—ã      0.230      0.001\n",
       "10602              –í–∏–∫—Ç–æ—Ä–∏—è –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤–Ω–∞      0.002      0.000\n",
       "24851                               –î–µ–Ω—å–≥–∏      0.019      0.003\n",
       "6289                                   —Å–±–ø      0.004      0.000\n",
       "13311                    –ú–µ–ª–∫–æ–≤ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä       0.000      0.000\n",
       "23338                            –Ω–∞ –≥–∞–∑–∑–∑–∑      0.003      0.001\n",
       "24694                                –ø—Ä–æ–±–∞      0.000      0.001\n",
       "16272             —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –¥—Ä—É–∂–æ—á–µ–∫      0.968      0.001\n",
       "11057                        –°–∞—à–∞–ö–æ–∫ —à–ø–∞–ª–∞      0.000      0.004\n",
       "24102                          –ò–ª—å—è –ë–ª—É–∫–∞—á      0.000      0.000\n",
       "12774          –ö–∞—Ç—é—à–∫–∞, —Å–ø–∞—Å–∏–±–æ. —ç—Ç–æ –õ—é–¥–∞.      0.556      0.000\n",
       "11854                               –ø—Ä–∏–≤–µ—Ç      0.003      0.000\n",
       "6370                           –∑–∞ —Ñ—É—Ç–±–æ–ª–∫–∏      0.004      0.000\n",
       "4924                       –ü—Ä–æ–±–Ω—ã–π –ü–µ—Ä–µ–≤–æ–¥      0.000      0.000\n",
       "30899                                 –î–æ–ª–≥      0.003      0.000\n",
       "29459                    –¢–∞—Ç—å—è–Ω–∞ –§—ë–¥–æ—Ä–æ–≤–Ω–∞      0.013      0.000\n",
       "8070      –ó–∞ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤—É –ö–æ–Ω—Ç—É—Ä–Ω—ã–µ –ö–∞—Ä—Ç—ã      0.000      0.000\n",
       "17683    –í –≠—Ç–æ–º –ú–µ—Å—è—Ü–µ - –ø–æ —á–∞—Å—Ç—è–º. –ù. –ú.       0.007      0.000\n",
       "3010                               –¥–æ–ª–∂–æ–∫.      0.001      0.000\n",
       "4587         –ó–∞ –ø–∏—Ç–∞–Ω–∏–µ –ö—Ä—é–∫–æ–≤–∞  —á–µ–ª–æ–≤–µ–∫–∞       0.012      0.000\n",
       "7918                       –°–ø–∞—Å–∏–±–æ ##;##;       1.000      0.000\n",
       "24045                  –ó–∞ –ø—Ä—è–Ω–∏–∫–∏  –Ω–∞–±–æ—Ä–æ–≤      0.000      0.000\n",
       "11681                         –ì–æ—Ä–æ–¥–Ω—è–Ω—Å–∫–∏–π      0.005      0.000\n",
       "21258                          –ó–∞ –∫—Ä—ã–ª—å—è))      0.061      0.000\n",
       "1979                                 —Ç–∞–∫—Å–∏      0.000      0.000\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[['message', 'preds_pos', 'preds_neg']].sort_values(by='preds_pos', ascending=False).sample(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
